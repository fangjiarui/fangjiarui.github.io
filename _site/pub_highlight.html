<!DOCTYPE html>
<html lang="en">
  <head>
    <meta http-equiv="content-type" content="text/html; charset=utf-8" />
    <title>Jiarui Fang</title>

    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Le styles -->
    <link href="../../assets/css/bootstrap.min.css" rel="stylesheet">
    <link href="../../assets/css/bootstrap-responsive.min.css" rel="stylesheet">
    <link href="../../assets/css/yangqing.css" rel="stylesheet">
  </head>

  <body>

    <div class="container">
      <div class="row">
        <div class="span3 bs-docs-sidebar">
          <!-- We use a fancy nav bar if there is enough space -->
          <hr class="hidden-phone"/>
          <ul class="nav nav-list bs-docs-sidenav hidden-phone">
            <li><a href="../../index.html"><i class="icon-play"></i>Home</a></li>
            <li><a href="../../blogs_index.html"><i class="icon-play"></i>Research</a></li>
            <li><a href="../../assets/pdf/CV-jiarui-fang.pdf"><i class="icon-play"></i>CV</a></li>
            <li><a href="../../pub_highlight.html"><i class="icon-play"></i>Blogs</a></li>
          </ul>
          <div class="visible-phone">
            <a href="index.html"><i class="icon-play"></i>Home</a>
            <a href="blogs_index.html"><i class="icon-play"></i>Research</a
            <a href="assets/pdf/CV.pdf"><i class="icon-play"></i>CV</a>
            <a href="pub_highlight.html"><i class="icon-play"></i>Blogs</a>
          </div>
        </div>
        <div class="span9">
          <h4>1. High Performance Deep Learning System on GPU cluster</h4>

<div class="media">
  <!-- <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/swcaffelogo.png" width="96px" height="96px"/>
  </a> -->
  <div class="media-body">
    <p class="media-heading">
      <i><strong>RedSync: Reducing synchronization bandwidth for distributed deep learning training system</strong></i><br />
      RedSync: Reducing synchronization bandwidth for distributed deep learning training system,
      Fang Jiarui, Fu Haohuan, Yang Guangwen, Hsieh Cho-Jui,
      Journal of Parallel and Distributed Computing 133 (JPDC), 30-39
      <a href="https://arxiv.org/pdf/1808.04357.pdf">[PDF]</a>
    </p>
    <!-- <p class="abstract-text">
    
    </p>  -->
  </div>
</div>

<h4>2. High Performance Deep Learning System on Sunway TaihuLight Supercomputer</h4>
<div class="media">
  <div class="media-body">
    <p class="media-heading">
      <strong><i>swATOP: Automatically Optimizing Deep Learning Operators on SW26010 Many-Core Processor.</i></strong><br />
      Wei Gao*, Jiarui Fang*, Wenlai Zhao, Jinzhe Yang, Long Wang, Lin Gan, Haohuan Fu, Guangwen Yang. (*equal contribution), 
      swATOP: Automatically Optimizing Deep Learning Operators on SW26010 Many-Core Processor,
      Proceedings of the 48th International Conference on Parallel Processing (ICPP 2019).<br />
      <a href="https://fangjiarui.github.io/assets/pdf/ICPP2019.pdf">[PDF]</a>
    </p>
    <p class="media-heading">
      <strong><i>swCaffe: a Parallel Framework for Accelerating Deep Learning Applications on Sunway TaihuLight</i></strong><br />
      Jiarui Fang*, and Li, Liandeng* and Fu, Haohuan and Jiang, Jinlei and Zhao, Wenlai and He, Conghui and You, Xin and Yang, Guangwen. (* equal contribution), 
      IEEE Cluster (Cluster 2018), Belfast, UK, 2018.<br />
      <a href="https://arxiv.org/abs/1903.06934">[PDF]</a>
      <a href="https://github.com/feifeibear/SWCaffe">[Software]</a>

    </p>
    <p class="media-heading">
      <strong>swDNN: A Library for Accelerating Deep Learning Applications on Sunway TaihuLight Supercomputer</strong><br />
     Jiarui Fang, Haohuan Fu, et al., 31st IEEE International Parallel & Distributed Processing Symposium (IPDPS 2017), 2017.6<br />
      <a href="assets/pdf/swdnn-ipdps-2017.pdf">[PDF]</a>
      <a href="assets/pdf/swDNN-IPDPS-final-ppt.pdf">[PPT]</a>

      <a href="https://github.com/THUHPGC/swDNN">[Software]</a>
    </p>
    <!-- <p class="abstract-text">
    We report our work on swDNN, which is a highly- efficient library for accelerating deep learning applications. We derive a performance model that guides us in the process of identifying the most suitable approach for mapping the convolutional neural networks (CNNs) onto bottom hardware. By performing a systematic optimization that explores major factors, such as organization of convolution loops, blocking techniques, register data communication schemes, as well as reordering strategies for the two pipelines of instructions, we manage to achieve a double-precision performance over 1.6 Tflops for the convolution kernel, achieving 54% of the theoretical peak. Compared with Tesla K40m with cuDNNv5, swDNN results in 1.91-9.75x performance speedup in an evaluation with over 100 parameter configurations.
    </p>  -->
  </div>
</div>

<h4>3. High Performance Geophysicists Applications</h4>
<div class="media">
  <!-- <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/hipclogo.png" width="96px" height="96px"/>
  </a> -->
  <div class="media-body">
    <p class="media-heading">
      <i><strong>Cache-friendly Design for Complex Spatially-variable Coeffcient Stencils on Many-core Architectures</strong></i><br />
      Jiarui Fang, Haohuan Fu and Guangwen Yang. IEEE 23rd International Conference on High Performance Computing, Data, and Analytics (HiPC 2016), 2016.12
      <a href="assets/pdf/HiPC2016-finial-submit.pdf">[PDF]</a>
    </p>
    <p class="media-heading">
      <i></i><strong>Optimizing Complex Spatially-Variant Coe cient Stencils for Seismic Modeling on GPU.</strong></i><br />
      Jiarui Fang, Haohuan Fu, He Zhang, et al. IEEE 21st International Conference on Parallel and Distributed Systems(ICPADS 2015),
      <a href="assets/pdf/ICPADSfinial128.pdf">[PDF]</a>
    </p>
    <p class="media-heading">
      <i><strong>GPU-based explicit time evolution method.</strong></i>i><br />
      Jiarui Fang, Haohuan Fu, Guangwen Yang, et al The 84th Society of Exploration Geophysicists Technical Program Expanded Abstracts (SEG 2014),
      <a href="assets/pdf/ETE_GPU_finial.pdf">[PDF]</a>
    </p>
    <!-- <p class="abstract-text">
    Many-core architectures, such as the NVIDIA graphics processing unit and Intel Xeon Phi, 
    which are characterized by high computation resources but limited on-chip memory capacity, 
    have been used to significantly accelerate various computationally demanding tasks. 
    Stencil operators are naturally suitable for such architectures because of their parallel calculation patterns. 
    However, only simple stencils with points distributed along the axes and with constant coefficients have been fully investigated. 
    This study first provides insights into optimization strategies for stencils with complex shapes, including off-axial points and spatially variable coefficients. 
    Through our proposed stencil-decomposition schemes, we maintain read-only coefficients in on-chip caches to avoid unvectorized memory access. 
    To alleviate the resulting severe cache-starvation situation, a generalized cache-friendly design for many-core architecture is proposed. 
    It can reduce cache miss times and cache space consumption. 
    The proposed methodology significantly improves the performance of stencil operations in a real seismic imaging application and introduces a new option to write highly efficient memory-bound stencil-like loops.
    </p>  -->
  </div>
</div>
<!-- 
<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/ipdpslogo.png" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>Optimizing Complex Spatially-Variant Coe cient Stencils for Seismic Modeling on GPU.</strong><br />
      Jiarui Fang, Haohuan Fu, He Zhang, et al. IEEE 21st International Conference on Parallel and Distributed Systems(ICPADS),
      <a href="assets/pdf/ICPADSfinial128.pdf">[PDF]</a>
    </p>
    <p class="abstract-text">
    The Explicit Time Evolution (ETE) method is an in- novative Finite-Difference (FD) type method to simulate the wave propagation in acoustic media with higher spatial and temporal accuracy. However, different from FD, it is difficult to achieve an efficient GPU design because of the poor memory access patterns caused by the off-axis points and spatially-variant coefficients. In this paper, we present a set of new optimization strategies for ETE stencils according to the memory hierarchy of NVIDIA GPU. To handle the problem caused by the complexity of the stencil shapes, we design a one-to-multi updating scheme for shared memory usage. To alleviate the performance damage resulted from the poor memory access pattern of reading spatially-variant coefficients, we propose a stencil decomposition method to reduce un-coalesced global memory access. Based on the state-of-the-art GPU architecture, combining with existing spatial and temporal stencil blocking schemes, we manage to achieve 9.6x and 9.9x speedups compared with a well-tuned 12-core CPUs version for 37-point and 73-point ETE stencils, respectively. Compared with a well-tuned MIC version, the best speedups for the 2 type stencils are 3.7x and 4.7x. Our designs leads to an ETE method that is 31.2x faster than conventional CPU-FD method and make it a practical seismic imaging technology.
    </p> 
  </div>
</div>

<div class="media">
  <a class="pull-left" href="#">
    <img class="media-object" src="assets/img/seglogo.jpg" width="96px" height="96px"/>
  </a>
  <div class="media-body">
    <p class="media-heading">
      <strong>GPU-based explicit time evolution method.</strong><br />
      Jiarui Fang, Haohuan Fu, Guangwen Yang, et al The 84th Society of Exploration Geophysicists Technical Program Expanded Abstracts, 2015.10
      <a href="assets/pdf/ETE_GPU_finial.pdf">[PDF]</a>
    </p>
    <p class="abstract-text">
    Finite-difference (FD) methods have long been among the most popular solutions for RTM and FWI. Compared with FD methods, the Explicit Time Evolution (ETE) method is able to simulate the wave propagation in acoustic media with higher spatial and temporal accuracy, at the cost of a more complicated memory access pattern. Similar to FD, ETE performs a stencil operation on every grid point, except that the coefficients of the stencil change spatially with the velocity parameter of that position. While FD methods already have highly-efficient designs on GPU platforms, in ETE, the sharp velocity discontinuities can result in un-coalesced memory access patterns. Moreover, the increased number of involved off-axis points in the stencil and the increased number of different coefficients bring more pressure for the fast buffers and memory in the GPU. To solve these issues, in this paper, we decompose the complex stencil into a number of sub-components, so as to form a better memory access pattern for coefficients and to simplify the calculation of stencil operations. Finally, we combine the decomposition scheme with 2.5D spatial and 1D temporal blocking optimizations. With one K20 GPU card, we manage to achieve 5.5x speedup compared against 12 cores Intel E5- E5645 CPU.
    </p> 
  </div>
</div>


 -->

        </div>
      </div>
    </div>

    <!-- Footer
    ================================================== -->
    <hr>
    <footer class="footer">
    <div class="container">
      <div class="row">
        <div class="span12">
          <p>&copy; Jiarui Fang 2017</p>
        </div>
      </div>
    </div>
    </footer>



    <!-- Le javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
<!--
    <script type="text/javascript" src="http://platform.twitter.com/widgets.js"></script>
    <script src="//ajax.googleapis.com/ajax/libs/jquery/1.9.1/jquery.min.js"></script>
    <script src="assets/js/bootstrap-transition.js"></script>
    <script src="assets/js/bootstrap-alert.js"></script>
    <script src="assets/js/bootstrap-modal.js"></script>
    <script src="assets/js/bootstrap-dropdown.js"></script>
    <script src="assets/js/bootstrap-scrollspy.js"></script>
    <script src="assets/js/bootstrap-tab.js"></script>
    <script src="assets/js/bootstrap-tooltip.js"></script>
    <script src="assets/js/bootstrap-popover.js"></script>
    <script src="assets/js/bootstrap-button.js"></script>
    <script src="assets/js/bootstrap-collapse.js"></script>
    <script src="assets/js/bootstrap-carousel.js"></script>
    <script src="assets/js/bootstrap-typeahead.js"></script>
    <script src="assets/js/bootstrap-affix.js"></script>
    <script src="assets/js/holder/holder.js"></script>
    <script src="assets/js/application.js"></script>
-->
  </body>
</html>
