---
layout: default
title: Jiarui Fang
---
<div><!-- Do I want to show a pic on the phone screen?-->
<div class="rightphoto"><img src="assets/img/fangjiaruiv3.jpeg" alt="photo" width="200px" /></div>
<div class="leftpad">
<h3>Jiarui Fang (方佳瑞)</h3>
<h5><a href="mailto:fangjiarui123@gmail.com">fangjiarui123 AT gmail.com</a></h5>
<p>
  I am currently a software engineer at ByteDance, leading the Volcano Machine Learning Platform (veMLP) team. Before joining ByteDance, I was a principal software engineer at Tencent.
</p>
<p>
  My passion for open-source has led me to initiate several impactful projects, including:
</p>
<ul>
  <li><b><a href="https://github.com/xdit-project/xDiT">xDiT</a></b>: A scalable inference engine for Diffusion Transformers (DiTs) on multi-GPUs.</li>
  <li><b><a href="https://github.com/hpcaitech/ColossalAI">ColossalAI</a></b>: A unified parallel training framework for large language models, which I led as CTO at HPCAI-Tech.</li>
  <li><b><a href="https://github.com/Tencent/PatrickStar">PatrickStar</a></b>: An efficient parallel training framework for large language models that is now a core component of ColossalAI. It is also used in training <a href="https://hunyuan.tencent.com/">Tencent Hunyuan LLM</a> in Tencent, and adopted by <a href="https://developer.aliyun.com/article/1610582">Pai-Megatron-Patch</a> in Alibaba Cloud.</li>
  <li><b><a href="https://github.com/Tencent/TurboTransformers">TurboTransformers</a></b>: A high-performance inference runtime for Transformers.</li>
</ul>
<p>
  From 2022 to 2023, I gained valuable experience in the startup world. I was a technical partner at LightYearAI, where I built and led a team of over 20 engineers focused on data curation for large language model pre-training. The company was acquired by Meituan just three months after its inception. Prior to that, I was the CTO of HPCAI-Tech for a whole year, a startup dedicated to open-source AI infrastructure at that time.
</p>
<p>
  Earlier in my career, I was a senior engineer at WeChat AI (Tencent), where I focused on improving the efficiency of AI applications through parallel computing. I also contributed to the development of foundational modules in the WeChat app, such as the <a href="https://z.weixin.qq.com/">WeChat Input Method Engine</a>. In recognition of my contributions to open-source collaboration, I was honored to receive Tencent's highest individual award in 2021.
</p>
<p>
  I earned my Ph.D. in Computer Science from Tsinghua University in 2019, where I was advised by Prof. <a href="https://www.cs.tsinghua.edu.cn/info/1107/3507.htm">Guangwen Yang</a> and Prof. <a href="http://47.94.243.94/mediawiki/index.php/Haohuan_Fu">Haohuan Fu</a>. My doctoral research, titled <i>Parallel Deep Learning Training System on Sunway TaihuLight</i>, focused on applying high-performance computing to scientific applications. I also had the privilege of serving as a visiting scholar at the University of California, Davis, under the supervision of <a href="https://web.cs.ucla.edu/~chohsieh/">Cho-Jui Hsieh</a> from 2018 to 2019. I received my B.S. in Computer Science from Beijing University of Posts and Telecommunications in 2014.
</p>
<p>
  Outside of work, I enjoy jogging, football, swimming, and table tennis. You can follow my work and interests on <a href="https://www.zhihu.com/people/feifeibear">知乎</a> and <a href="https://github.com/feifeibear">Github</a>.
</p>
</div>
</div>
<p>&nbsp;</p>
<!--<a href="projects.html"> [Unpublished Projects]</a>-->
<p>&nbsp;&nbsp;</p>
<!--
 *** Software ***

<h3>
  <a name='software'></a> Software 
</h3>

{% include software.html %}
--><!--
 *** Teaching ***

<h3>
  <a name='teaching'></a> Teaching (GSI)
</h3>

{% include teaching.html %}
-->

<body>
<script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=300&t=tt&d=IUauw8ewM48TcdV7vaC71gwnoW5ILE5GvL1ZyMOfBh0&co=2d78ad&cmo=3acc3a&cmn=ff5353&ct=ffffff"></script>
</body>
